# Epic 2 Retrospective – Data Engine

**Epic:** 2  
**Epic Title:** Data Ingestion & Normalization Engine  
**Date:** 2025-11-21  
**Participants:** stefano (Developer / Project Lead), Bob (Scrum Master, AI assistant)

---

## 1. Epic Overview

Epic 2 delivered the data “brain” of the Arbitrage Finder App: a normalized arbitrage data model, centralized rate limiting, production and test provider adapters, a calibration harness, a golden dataset correctness gate, and a first pass at structured logging and observability.

- **2.1 – Adapter Pattern & Shared Types:** Canonical `ArbitrageOpportunity` and shared adapter contract wired through poller and calculator with schema validation and tests.
- **2.2 – Rate Limiter Implementation:** Central Bottleneck-based limiter in `poller.ts`, provider-aware backoff, and tests that enforce quota-respecting behaviour.
- **2.3 – Rate Limit Calibration & Stress Harness:** CI-friendly calibration loop with structured metrics and quota checks to keep 429s and unsafe configs under control.
- **2.4 – Production Adapter (Odds-API.io):** Live pre-calculated arbitrage feed mapped into the canonical model with region/sport filters and rate-limit integration.
- **2.5 – Test Adapter (The-Odds-API.com):** Raw-odds based local arbitrage detection using shared calculators and schemas, giving a safe non-production test bed.
- **2.6 – Golden Dataset & Arbitrage Correctness Tests:** P0 golden fixtures and a pure calculator entry point that now act as the main R-002 correctness gate.
- **2.7 – Logging & Observability Baseline:** Structured logging helper, adapter/poller instrumentation, heartbeat events, and initial secret-redaction tests.

Epic status: **Complete** (Stories 2.1–2.6 done, 2.7 in review but functionally in place; retrospective recorded).

---

## 2. What Went Well

1. **Strong architectural alignment with Epic 1 and PRD**
   - Adapters, poller, calculator, and types all line up with `_bmad-output/architecture.md` and reuse the credentials and settings boundaries from Epic 1 rather than introducing one-off paths.
2. **Contracts and schemas enforced early**
   - The canonical `ArbitrageOpportunity` type and Zod schemas gave a clear contract for all 2.x stories, reducing ambiguity and making tests more meaningful.
3. **Risk-first focus on R-001 and R-002**
   - Rate limiting, calibration, and golden dataset tests directly target the highest-risk areas (quotas and arbitrage correctness) instead of treating them as afterthoughts.
4. **Good test coverage for the data engine**
   - Node test suites cover adapter wiring, rate limiting, calibration, golden datasets, and logging hygiene; regressions in the “brain” are now easier to catch.
5. **Separation of concerns held up well**
   - Poller owns rate limiting and scheduling, calculator stays pure, and adapters focus on mapping—this separation should scale into Epic 3 and 4 without major refactors.

---

## 3. What Was Challenging

1. **Complexity of rate limiting and backoff**
   - Translating PRD quotas into concrete Bottleneck settings, modelling 429 behaviour, and keeping tests deterministic required careful tuning and introduced mental overhead.
2. **Balancing production vs test providers**
   - Keeping production and test adapters consistent while preserving their different behaviours (pre-calculated vs raw odds) added complexity to mapping, filtering, and testing.
3. **Golden dataset and correctness harness design**
   - Designing fixtures that are small, readable, and still realistic enough to catch regressions took time; encoding R-002 invariants without flaky edge cases was non-trivial.
4. **Logging without leaking secrets or noise**
   - Instrumenting adapters and poller with structured logs while avoiding secrets and overly verbose payloads forced several iterations on logger design and tests.
5. **End-to-end mental model**
   - Keeping a clear picture of how credentials, adapters, rate limiting, calibration, golden tests, and logging all interact made the learning curve steeper for this epic.

---

## 4. Key Lessons Learned

1. **Canonical types plus schemas are worth the upfront cost**
   - The combination of `ArbitrageOpportunity` + Zod schemas + pure calculator entry points significantly reduced ambiguity and made future stories easier to reason about.
2. **Centralized rate limiting must be treated as a platform concern**
   - Putting all provider I/O behind `scheduleProviderRequest` in `poller.ts` kept the surface area small and testable; any new provider must plug into this same path.
3. **Golden datasets make correctness tangible**
   - Having small, named fixtures and explicit R-002 tests improved confidence and made “correctness” a concrete, repeatable check instead of a vague property.
4. **Logging design should follow clear conventions**
   - A single logger with consistent `context`/`operation` fields and redaction rules avoids ad-hoc patterns and will make future dashboards and troubleshooting simpler.
5. **Reuse security and boundary patterns from earlier epics**
   - Reapplying Epic 1’s “no secrets in logs” and “single credentials boundary” patterns prevented avoidable security drift as the data engine became more complex.

---

## 5. Action Items

**AI-01 – Tighten rate-limit calibration scenarios**  
- **Description:** Add a few focused calibration profiles (e.g., “realistic production usage”, “stress test for new provider”) with documented expectations so quota issues are easier to spot and interpret from logs.  
- **Owner:** stefano  
- **When to apply:** Before introducing additional providers or changing quotas.

**AI-02 – Expand logging and heartbeat consumption plan**  
- **Description:** Sketch how `poller.heartbeat` and adapter logs will be wired into the Epic 3 dashboard (system status, provider health, staleness indicators) to avoid rework later.  
- **Owner:** stefano  
- **When to apply:** During Epic 3 design and early story prep.

**AI-03 – Document provider-specific mapping assumptions**  
- **Description:** Capture key mapping assumptions for both Odds-API.io and The-Odds-API.com (fields used, expected invariants, known quirks) in a short section of `_bmad-output/architecture.md` or a dedicated provider notes doc.  
- **Owner:** stefano  
- **When to apply:** Before making major changes to adapters or adding new feeds.

**AI-04 – Reuse golden dataset in more tests**  
- **Description:** Look for additional adapter, poller, or pipeline tests that could reuse the golden fixtures instead of introducing new inline examples, keeping correctness checks centralized.  
- **Owner:** stefano  
- **When to apply:** As Epic 3/4 stories start touching the data engine.

**AI-05 – Close the loop on 2.7 logging review status**  
- **Description:** Resolve the remaining Story 2.7 review notes (e.g., integration test mismatch) and mark logging baseline as fully complete before Epic 3 relies on it.  
- **Owner:** stefano  
- **When to apply:** Early in Epic 3, before dashboard stories depend on logging.

---

## 6. Next Epic Preparation (Epic 3 – Dashboard & UX)

- **Focus:** Turn the now-validated data engine into a trustworthy, responsive dashboard experience with clear status, staleness, and filtering.
- **Dependencies from Epic 2:**
  - `ArbitrageOpportunity` and related schemas remain the single source of truth for opportunities.
  - Poller + adapters must be treated as the only path for live data; UI should not bypass these boundaries.
  - Golden dataset and R-002 tests must stay green as new UX features land.
  - Logging and heartbeat data will back system and provider status indicators.
- **Readiness Check:**
  - All core Epic 2 stories (2.1–2.6) are **done**, and logging baseline is implemented with tests.
  - This retrospective document is saved and can be referenced when shaping Epic 3 dashboard and UX stories.

Bob (Scrum Master): "Epic 2 gave you a robust, testable data engine. The key for Epic 3 is to treat this engine as a stable platform—reuse its contracts, logs, and correctness guards instead of re-solving data problems in the UI."

